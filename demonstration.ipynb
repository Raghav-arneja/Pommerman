{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-AGENT LEARNING WITH (B)POMMERMAN\n",
    "\n",
    "In this notebook we demonstrate our key results to the Pommerman Project.\n",
    "\n",
    "Usage: Adjust the global settings before running this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed settings\n",
    "LAYERS = 13  #Upgrade layers merged: layers -2 (Hard coded).\n",
    "\n",
    "### Changeable settings\n",
    "\n",
    "EPISODES = 1000000\n",
    "LR = 1e-3\n",
    "RANDOMSTART = True  # randomize agent starting location.\n",
    "CENTERED    = True  # reduces layers by 1 (my position)\n",
    "BLASTPAT    = True  # reduces layers by 2 (bomb map, bomb strength, bomb life is merged)\n",
    "\n",
    "# reward shaping:\n",
    "SAREWARD    = True   # reward shaping using Simple Agent (Off policy, Simple agent in control)\n",
    "PXREWARD    = False  # Reward shaping using the Pommerman X (link) reward shaping\n",
    "\n",
    "#select network:\n",
    "CONVNET          = False # convolutional\n",
    "TENSORFLOWRESNET = False  # layers -3 (ammo, blast_strength & can_kick moved to singular values)\n",
    "TFRWIGHTMAN      = True\n",
    "\n",
    "\n",
    "#Select algorithm:\n",
    "PPO = True\n",
    "DQN = False\n",
    "\n",
    "#Exploration (off policy)\n",
    "EXPLORE = False\n",
    "\n",
    "#BatchNorm Hyperparams:\n",
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "#Calculate model layers based on params\n",
    "if CENTERED:\n",
    "    LAYERS -= 1\n",
    "if BLASTPAT:\n",
    "    LAYERS -= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to have tensorflow and tensorforce installed\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility\n",
    "from pommerman.constants import *\n",
    "from tensorforce.agents import DQNAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.core.networks.network import LayerBasedNetwork\n",
    "from tensorforce.core.networks.layer import TFLayer\n",
    "from tensorforce.core.networks import Layer\n",
    "from tensorforce.core.networks import Network\n",
    "from tensorflow import nn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import csv\n",
    "import copy\n",
    "       \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TENSORFLOW RESNET NETWORK \n",
    "\n",
    "inspired by:\n",
    "https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res-net function with batch nomalization\n",
    "def res(inputs, filters, training, strides):\n",
    "\tshortcut = inputs\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=strides)\n",
    "\tinputs = tf.layers.batch_normalization(inputs=inputs, axis=3,momentum=_BATCH_NORM_DECAY,epsilon=_BATCH_NORM_EPSILON, center=True,scale=True, training=training, fused=True)\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=1)\n",
    "\tinputs = tf.layers.batch_normalization(inputs=inputs, axis=3,momentum=_BATCH_NORM_DECAY,epsilon=_BATCH_NORM_EPSILON, center=True,scale=True, training=training, fused=True)\t\n",
    "\tinputs += shortcut\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\treturn inputs\n",
    "\n",
    "#res-net function with batch nomalization\n",
    "#no shortcut, for input data\n",
    "def resnsc(inputs, filters, training, strides):\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=strides)\n",
    "\tinputs = tf.layers.batch_normalization(inputs=inputs, axis=3,momentum=_BATCH_NORM_DECAY,epsilon=_BATCH_NORM_EPSILON, center=True,scale=True, training=training, fused=True)\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=1)\n",
    "\tinputs = tf.layers.batch_normalization(inputs=inputs, axis=3,momentum=_BATCH_NORM_DECAY,epsilon=_BATCH_NORM_EPSILON, center=True,scale=True, training=training, fused=True)\t\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\treturn inputs\n",
    "\n",
    "#res-net function without batch nomalization\n",
    "def resnbn(inputs, filters, training, strides):\n",
    "\tshortcut = inputs\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=strides)\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\tinputs = conv2d_fixed_padding(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=3, strides=1)\n",
    "\tinputs += shortcut\n",
    "\tinputs = tf.nn.relu(inputs)\n",
    "\treturn inputs\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, strides):\n",
    "\t\"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "\t# The padding is consistent and is based only on `kernel_size`, not on the\n",
    "\t# dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n",
    "\tif strides > 1:\n",
    "\t\tinputs = fixed_padding(inputs, kernel_size)\n",
    "\n",
    "\treturn tf.layers.conv2d(\n",
    "\t\t\tinputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "\t\t\tpadding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n",
    "\t\t\tkernel_initializer=tf.variance_scaling_initializer())\n",
    "\n",
    "class TFNetworkRES(Network):\n",
    "\t\tdef __init__(self, scope='TFNetworkRES', summary_labels=()):\n",
    "\t\t\t\tsuper(TFNetworkRES, self).__init__(\n",
    "\t\t\t\t\t\tscope=scope, summary_labels=summary_labels)\t\n",
    "\t\tdef tf_apply(self, x, internals, update, return_internals=False):\n",
    "\t\t\t\tx1 = x['state']\n",
    "                #get singular values from whole boards:\n",
    "\t\t\t\tkick = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-1)], [-1, 1, 1, 1]))\n",
    "\t\t\t\tblast = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-2)], [-1, 1, 1, 1]))\n",
    "\t\t\t\tammo = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-3)], [-1, 1, 1, 1]))\n",
    "                #cut off whole boards\n",
    "\t\t\t\tstate = tf.slice(x1, [0, 0, 0, 0], [-1, 11, 11, (LAYERS-3)])\n",
    "                #res-net\n",
    "\t\t\t\tstate = conv2d_fixed_padding(state, 64, 3, 1)\n",
    "\t\t\t\tstate = resnsc(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = res(state, 64, True, 1)\n",
    "\t\t\t\tstate = tf.contrib.layers.flatten(state)\n",
    "                ## add singular values before dense layer\n",
    "\t\t\t\tstate = tf.concat([state, kick,blast,ammo], axis=1)\n",
    "\t\t\t\tstate = tf.layers.dense(state,3000,activation=tf.nn.relu)\n",
    "\t\t\t\tstate = tf.layers.dense(state,512,activation=tf.nn.relu)\n",
    "\t\t\t\tstate = tf.nn.softmax(state)\n",
    "\n",
    "\t\t\t\t# Combination\n",
    "\t\t\t\tif return_internals:\n",
    "\t\t\t\t\t\treturn\tstate, list() #tf.multiply(image, caption), list()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\treturn\tstate # tf.multiply(image, caption)\n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basline Network architecture\n",
    "comparison to state of the art / baseline,\n",
    "based on rwightman: https://github.com/rwightman/pytorch-pommerman-rl                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNetworkRWI(Network):\n",
    "\n",
    "\t\tdef __init__(self, scope='TFNetworkRWI', summary_labels=()):\n",
    "\t\t\t\tsuper(TFNetworkRWI, self).__init__(\n",
    "\t\t\t\t\t\tscope=scope, summary_labels=summary_labels)\t\n",
    "\t\tdef tf_apply(self, x, internals, update, return_internals=False):\n",
    "\t\t\t\tx1 = x['state']\n",
    "                #get singular values from whole boards:\n",
    "\t\t\t\tkick = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-1)], [-1, 1, 1, 1]))\n",
    "\t\t\t\tblast = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-2)], [-1, 1, 1, 1]))\n",
    "\t\t\t\tammo = tf.contrib.layers.flatten(tf.slice(x1, [0, 0, 0, (LAYERS-3)], [-1, 1, 1, 1]))\n",
    "                #cut off whole boards\n",
    "\t\t\t\tstate = tf.slice(x1, [0, 0, 0, 0], [-1, 11, 11, (LAYERS-3)])\n",
    "                \n",
    "\t\t\t\tstate = tf.layers.conv2d(state, 64, 3, 1,'SAME')  #11x11\n",
    "\t\t\t\tstate = tf.layers.conv2d(state, 64, 3, 1,'SAME')  #11x11\n",
    "\t\t\t\tstate = tf.layers.conv2d(state, 64, 3, 1,'VALID') #9x9\n",
    "\t\t\t\tstate = tf.layers.conv2d(state, 64, 3, 1,'VALID') #7x7\n",
    "\t\t\t\tstate = tf.contrib.layers.flatten(state)\n",
    "\t\t\t\tstate = tf.layers.dense(state,1024,activation=tf.nn.relu)     \n",
    "\t\t\t\tstate = tf.layers.dense(state,512,activation=None)                    \n",
    "\n",
    "\t\t\t\tnumeric = tf.concat([kick,blast,ammo], axis=1)   \n",
    "\t\t\t\tnumeric = tf.layers.dense(numeric,128,activation=tf.nn.relu)                 \n",
    "\t\t\t\tnumeric = tf.layers.dense(numeric,128,activation=tf.nn.relu) \n",
    "                \n",
    "\t\t\t\tstate = tf.concat([state,numeric], axis=1)\n",
    "\t\t\t\t# Combination\n",
    "\t\t\t\tif return_internals:\n",
    "\t\t\t\t\t\treturn\tstate, list() #tf.multiply(image, caption), list()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\treturn\tstate # tf.multiply(image, caption)\n",
    "\t\t\t\t\t    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONVNET:\n",
    "\tnet =  [dict(type='conv2d', size=64, window=3, stride=1, padding='SAME', activation='relu'),\n",
    "\t\t\tdict(type='conv2d', size=64, window=3, stride=1, padding='SAME', activation='relu'),\n",
    "\t\t\tdict(type='conv2d', size=64, window=3, stride=1, padding='SAME', activation='relu'),\n",
    "\t\t\tdict(type='conv2d', size=64, window=3, stride=1, padding='SAME', activation='relu'),\n",
    "\t\t\tdict(type='flatten'),\n",
    "\t\t\tdict(type='dense', size = 7744),\n",
    "\t\t\tdict(type='dense', size = 512),        \n",
    "\t\t\tdict(type='nonlinearity', name='softmax')]     \n",
    "            \n",
    "if TENSORFLOWRESNET:\n",
    "    net = TFNetworkRES\n",
    "\n",
    "if TFRWIGHTMAN:\n",
    "    net = TFNetworkRWI    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "\t ex = dict(\n",
    "\t\t\ttype= 'epsilon_anneal', #'epsilon_decay',\n",
    "\t\t\tinitial_epsilon=1.0,\n",
    "\t\t\tfinal_epsilon=0.001,\n",
    "\t\t\ttimesteps= (int(EPISODES * 0.7))\n",
    "\t\t\t)\n",
    "else:\n",
    "\t ex = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PPO:\n",
    "\tagent = PPOAgent(\n",
    "\tstates=dict(type='float', shape=(11,11,LAYERS)),\n",
    "\tactions=dict(type='int',\tnum_actions=env.action_space.n),\n",
    "\tbatching_capacity=1000,\n",
    "\tnetwork=net, #CustomTFLayerNetwork,\n",
    "\tactions_exploration=ex,\n",
    "\tstep_optimizer=dict(\n",
    "\t\ttype='adam',\n",
    "\t\tlearning_rate=LR\n",
    "\t\t)\n",
    "    )\n",
    "\n",
    "        \n",
    "\n",
    "if DQN:\n",
    "\tagent = DQNAgent(\n",
    "\tstates=dict(type='float', shape=(11,11,LAYERS)),\n",
    "\tactions=dict(type='int', num_actions=env.action_space.n),\n",
    "\tnetwork=net,\n",
    "\tbatching_capacity=1000,\n",
    "\tactions_exploration=ex,\n",
    "\toptimizer=dict(\n",
    "\t\ttype='adam',\n",
    "\t\tlearning_rate=LR\n",
    "\t\t)        \n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate Tensorforce Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "\tdef act(self, obs, action_space):\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 3 agents\n",
    "agents = []\n",
    "\n",
    "agents.append(SimpleAgent(config[\"agent\"](0, config[\"game_type\"])))\n",
    "agents.append(RandomAgent(config[\"agent\"](1, config[\"game_type\"])))\n",
    "agents.append(RandomAgent(config[\"agent\"](2, config[\"game_type\"])))\n",
    "\n",
    "# Add Traning Agent\n",
    "agent_id = 3\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add all agents to the environment\n",
    "env.set_agents(agents)\n",
    "\n",
    "# Define the agent that is trained\n",
    "env.set_training_agent(agents[agent_id].agent_id)\n",
    "\n",
    "# initiate game\n",
    "env.set_init_game_state(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REWARD SHAPING\n",
    "inspired by: Pommerman-x - https://github.com/papkov/pommerman-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBetween(a, b, c):\n",
    "\t\tcrossproduct = (c.y - a.y) * (b.x - a.x) - (c.x - a.x) * (b.y - a.y)\n",
    "\t\tepsilon = 0.0001\n",
    "\t\t# compare versus epsilon for floating point values, or != 0 if using integers\n",
    "\t\tif abs(crossproduct) > epsilon:\n",
    "\t\t\t\treturn False\n",
    "\n",
    "\t\tdotproduct = (c.x - a.x) * (b.x - a.x) + (c.y - a.y)*(b.y - a.y)\n",
    "\t\tif dotproduct < 0:\n",
    "\t\t\t\treturn False\n",
    "\n",
    "\t\tsquaredlengthba = (b.x - a.x)*(b.x - a.x) + (b.y - a.y)*(b.y - a.y)\n",
    "\t\tif dotproduct > squaredlengthba:\n",
    "\t\t\t\treturn False\n",
    "\n",
    "\t\treturn True\n",
    "\n",
    "class Point:\n",
    "\t\tdef __init__(self, x, y):\n",
    "\t\t\t\tself.x = x\n",
    "\t\t\t\tself.y = y\n",
    "\t\t\n",
    "\t\tdef scale(self, s):\n",
    "\t\t\t\tself.x *= s\n",
    "\t\t\t\tself.y *= s\n",
    "\t\t\t\treturn self\n",
    "\n",
    "\t\t\t\n",
    "class RewardShaping:\n",
    "\t\t\"\"\" It takes care of everything just add in the environment loop and make\n",
    "\t\t\t\tsure that you reset it either by creating a new calling reset at the end\n",
    "\t\t\t\tof each episode otherwise last observation from prev episode is used which\n",
    "\t\t\t\tcan raise some errors!\n",
    "\t\t\"\"\"\n",
    "\t\tdef __init__(self, obs_prev = None, action_prev = None):\n",
    "\t\t\t\tself.reset(obs_prev, action_prev)\n",
    "\t\t\t\t\n",
    "\t\tdef reset(self, obs_prev = None, action_prev = None):\n",
    "\t\t\t\tself.obs_prev = obs_prev\n",
    "\t\t\t\tself.action_prev = action_prev\n",
    "\t\t\t\t# for debug purpose\n",
    "\t\t\t\tself.obs_cur = None\n",
    "\t\t\t\tself.action_cur = None\n",
    "\t\t\t\tself.conseqActionCounter = 0\n",
    "\t\t\t\tself.dist2bombs_prev = 0\n",
    "\t\t\t\tself.notUsingAmmoCount = 0\n",
    "\t\t\t\t# catch enemy\n",
    "\t\t\t\tself.closestEnemyIdPrev = -1\n",
    "\t\t\t\tself.closestEnemyDistPrev = float(\"inf\")\n",
    "\t\t\t\t\n",
    "\t\tdef shape_it(self, obs_now, action_now):\n",
    "\t\t\t\t\"\"\" Shape the reward based on the current and previous observation\n",
    "\t\t\t\t\t\tinput: current observation, current action, and received reward\n",
    "\t\t\t\t\t\toutput: the shaped reward, sum of each criteria for the final reward\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tself.obs_cur = obs_now\n",
    "\t\t\t\tself.action_cur = action_now\n",
    "\t\t\t\tif self.obs_prev is None:\n",
    "\t\t\t\t\t\t#print(\"first iteration\")\n",
    "\t\t\t\t\t\tself.obs_prev = obs_now\n",
    "\t\t\t\t\t\tself.action_prev = action_now\n",
    "\t\t\t\t\t\treturn 0, None\n",
    "\t\t\t\t#\n",
    "\t\t\t\treward_tmp = {}\n",
    "\t\t\t\t# REWARD VALUES (NB: some of them used as factors not directly!)\n",
    "\t\t\t\tMOBILITY_RWRD = 0.1\n",
    "#\t\t\t\t CONSEQ_ACT_RWRD = -0.5\n",
    "\t\t\t\tCONSEQ_ACT_RWRD = -0.0001\n",
    "\t\t\t\t\n",
    "\t\t\t\tPLNT_BOMB_NEAR_WOOD_RWRD = 0.05\n",
    "\t\t\t\tPLNT_BOMB_NEAR_ENEM_RWRD = 0.1\n",
    "#\t\t\t\t ON_FLAMES_RWRD = -0.8\n",
    "\t\t\t\tON_FLAMES_RWRD = -0.0001\n",
    "\t\t\t\t\n",
    "\t\t\t\tINCRS_DIST_WITH_BOMBS_RWRD = 0.05\n",
    "\t\t\t\tPICKED_POWER_RWRD = 0.1\n",
    "#\t\t\t\t CATCH_ENEMY_RWRD = 0.1\n",
    "\t\t\t\tCATCH_ENEMY_RWRD = 0.001\n",
    "\t\t\t\t#\n",
    "\t\t\t\t# movement: + for mobility\n",
    "\t\t\t\tpose_t = np.array(obs_now['position']) # t\n",
    "\t\t\t\tpose_tm1 = np.array(self.obs_prev['position']) # t-1\n",
    "\t\t\t\tmoveDist = np.linalg.norm(pose_t-pose_tm1)\n",
    "\t\t\t\treward_tmp['mobility'] = MOBILITY_RWRD if moveDist > 0 else 0 # give reward\n",
    "\t\t\t\t#\n",
    "\t\t\t\t# consequative action: - for conseq actions\n",
    "\t\t\t\tif self.action_prev == action_now:\n",
    "\t\t\t\t\t\tself.conseqActionCounter += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.conseqActionCounter = 0\n",
    "\t\t\t\tif self.conseqActionCounter > 11:\n",
    "\t\t\t\t\t\treward_tmp['conseqact'] = CONSEQ_ACT_RWRD\n",
    "\t\t\t\t# keeping ammo: - for not using its ammo\n",
    "\t\t\t\tif obs_now['ammo'] == self.obs_prev['ammo']:\n",
    "\t\t\t\t\t\tself.notUsingAmmoCount += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.notUsingAmmoCount = 0\n",
    "\t\t\t\tif self.notUsingAmmoCount > 11:\n",
    "\t\t\t\t\t\treward_tmp['ammousage'] = CONSEQ_ACT_RWRD\n",
    "\t\t\t\t#\n",
    "\t\t\t\t# plant a bomb: + based on value of the bombing position\n",
    "\t\t\t\tbombs_pose = np.argwhere(obs_now['bomb_life'] != 0)\n",
    "\t\t\t\tif obs_now['ammo'] < self.obs_prev['ammo']:\n",
    "\t\t\t\t\t\tsurroundings = [(-1,1),(0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0)]\n",
    "\t\t\t\t\t\tmybomb_pose = self.obs_prev['position'] # equal to agent previous position\n",
    "\t\t\t\t\t\t# validate if the bomb actually exists there\n",
    "\t\t\t\t\t\tfound_the_bomb = False\n",
    "\t\t\t\t\t\tfor bp in bombs_pose:\n",
    "\t\t\t\t\t\t\t\tif np.equal(bp, mybomb_pose).all():\n",
    "\t\t\t\t\t\t\t\t\t\tfound_the_bomb = True\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\tassert found_the_bomb # end of validation\n",
    "\t\t\t\t\t\tnr_woods = 0\n",
    "\t\t\t\t\t\tnr_enemies = 0\n",
    "\t\t\t\t\t\tfor p in surroundings:\n",
    "\t\t\t\t\t\t\t\tcell_pose = (mybomb_pose[0] + p[0], mybomb_pose[1] + p[1])\n",
    "\t\t\t\t\t\t\t\tif cell_pose[0] > 10 or cell_pose[1] > 10: # bigger than board size\n",
    "\t\t\t\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\t\t\t#print(obs_now['board'][cell_pose])\n",
    "\t\t\t\t\t\t\t\tnr_woods += obs_now['board'][cell_pose] == Item.Wood.value\n",
    "\t\t\t\t\t\t\t\tnr_enemies += obs_now['board'][cell_pose] in [e.value for e in obs_now['enemies']]\n",
    "\t\t\t\t\t\t#print(\"nr woods: \", nr_woods)\n",
    "\t\t\t\t\t\t#print(\"nr enemies: \", nr_enemies)\n",
    "\t\t\t\t\t\tassert nr_woods + nr_enemies < 10\n",
    "\t\t\t\t\t\treward_tmp['plantbomb'] = \\\n",
    "\t\t\t\t\t\t\t\tPLNT_BOMB_NEAR_WOOD_RWRD * nr_woods \\\n",
    "\t\t\t\t\t\t\t\t+ PLNT_BOMB_NEAR_ENEM_RWRD * nr_enemies # give reward\n",
    "\t\t\t\t#\n",
    "\t\t\t\t# on Flames: - if agent on any blast direction \n",
    "\t\t\t\tfor bp in bombs_pose:\n",
    "\t\t\t\t\t\tdef rot_deg90cw(point):\n",
    "\t\t\t\t\t\t\t\tnew_point = [0, 0]\n",
    "\t\t\t\t\t\t\t\tnew_point[0] = point[1]\n",
    "\t\t\t\t\t\t\t\tnew_point[1] = -point[0]\n",
    "\t\t\t\t\t\t\t\treturn new_point\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t#print(type(bp))\n",
    "\t\t\t\t\t\tfactor = 1/obs_now['bomb_life'][tuple(bp)] # inverse of time left\n",
    "\t\t\t\t\t\tblast_strength = obs_now['bomb_blast_strength'][tuple(bp)]\n",
    "\n",
    "\t\t\t\t\t\t# blast directions\n",
    "\t\t\t\t\t\tblast_N = Point(0,1).scale(blast_strength)\n",
    "\t\t\t\t\t\tblast_S = Point(0,-1).scale(blast_strength)\n",
    "\t\t\t\t\t\tblast_W = Point(-1,0).scale(blast_strength)\n",
    "\t\t\t\t\t\tblast_E = Point(1,0).scale(blast_strength)\n",
    "\n",
    "\t\t\t\t\t\t# agent on blast direction?\n",
    "\t\t\t\t\t\tbpPose = rot_deg90cw(bp)\n",
    "\t\t\t\t\t\tmyPose = rot_deg90cw(obs_now['position'])\n",
    "\t\t\t\t\t\tmyPose = Point(myPose[0]-bpPose[0], myPose[1]-bpPose[1]) # my pose relative to the bomb!\n",
    "\t\t\t\t\t\tonBlastDirect = isBetween(blast_N, blast_S, myPose) or \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tisBetween(blast_W, blast_E, myPose)\n",
    "\t\t\t\t\t\tif onBlastDirect:\n",
    "\t\t\t\t\t\t\t\t#print(\"time: \", obs_now['bomb_life'][tuple(bp)])\n",
    "\t\t\t\t\t\t\t\t#print(\"on blast: \", factor)\n",
    "\t\t\t\t\t\t\t\treward_tmp['onflame'] = ON_FLAMES_RWRD * factor\n",
    "\t\t\t\t#\n",
    "\t\t\t\t# Bombs distance: + if total distance from bombs increased\n",
    "\t\t\t\tdist2bombs = 0\n",
    "\t\t\t\tfor bp in bombs_pose:\n",
    "\t\t\t\t\t\tdist2bombs += np.linalg.norm(obs_now['position']-bp)\n",
    "\t\t\t\tdist_delta = dist2bombs - self.dist2bombs_prev\n",
    "\t\t\t\tself.dist2bombs_prev = dist2bombs\n",
    "\t\t\t\t#print(dist_delta)\n",
    "\t\t\t\t# TODO: this may not be good if the delta oscillates all the time\n",
    "\t\t\t\tif (dist_delta > 0 and moveDist):\n",
    "\t\t\t\t\t\treward_tmp['bombsdistance'] = dist_delta * INCRS_DIST_WITH_BOMBS_RWRD\n",
    "\t\t\t\t# picked power: + for every picked power\n",
    "\t\t\t\tpotentialPower = self.obs_prev['board'][obs_now['position']]\n",
    "\t\t\t\tpicked_power = (potentialPower == Item.ExtraBomb.value) or\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t (potentialPower == Item.IncrRange.value) or\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t (potentialPower == Item.Kick.value)\n",
    "\t\t\t\tif picked_power:\n",
    "\t\t\t\t\t\treward_tmp['pickedpower'] = PICKED_POWER_RWRD\n",
    "\t\t\t\t\n",
    "\t\t\t\t# catch enemy: + if closing distance with the nearest enemy\n",
    "\t\t\t\tdef closestEnemy():\n",
    "\t\t\t\t\t\tmyPose = obs_now['position']\n",
    "\t\t\t\t\t\tclosestEnemyId = -1\n",
    "\t\t\t\t\t\tclosestEnemyDist = float(\"inf\")\n",
    "\t\t\t\t\t\tfor e in obs_now['enemies']:\n",
    "\t\t\t\t\t\t\t\tenemyPose = np.argwhere(obs_now['board'] == e.value)\n",
    "\t\t\t\t\t\t\t\tif len(enemyPose) == 0:\n",
    "\t\t\t\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\t\t\tdist2Enemy = np.linalg.norm(myPose-enemyPose)\n",
    "\t\t\t\t\t\t\t\tif dist2Enemy <= closestEnemyDist:\n",
    "\t\t\t\t\t\t\t\t\t\tclosestEnemyId = e.value\n",
    "\t\t\t\t\t\t\t\t\t\tclosestEnemyDist = dist2Enemy\n",
    "\t\t\t\t\t\treturn closestEnemyId, closestEnemyDist\n",
    "\n",
    "\t\t\t\tclosestEnemyId_cur, closestEnemyDist_cur = closestEnemy()\n",
    "\t\t\t\tif self.closestEnemyIdPrev != closestEnemyId_cur:\n",
    "\t\t\t\t\t\tself.closestEnemyIdPrev = closestEnemyId_cur\n",
    "\t\t\t\t\t\tself.closestEnemyDistPrev = closestEnemyDist_cur\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tCATCHING_TRHE = 4 # consider catching when close at most this much to the enemy\n",
    "\t\t\t\t\t\tif closestEnemyDist_cur < self.closestEnemyDistPrev and\\\n",
    "\t\t\t\t\t\t\t\tclosestEnemyDist_cur < CATCHING_TRHE:\n",
    "\t\t\t\t\t\t\t\treward_tmp['catchenemy'] = CATCH_ENEMY_RWRD\n",
    "\t\t\t\t\t\t\t\tself.closestEnemyDistPrev = closestEnemyDist_cur\n",
    "\t\t\t\t\t\tif closestEnemyDist_cur <= 1.1: # got that close\n",
    "\t\t\t\t\t\t\t\tself.closestEnemyDistPrev = float(\"inf\")\n",
    "\t\t\t\t#print(\"catching: \", closestEnemyIdPrev)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# update previous obs and action\n",
    "\t\t\t\tself.obs_prev = obs_now\n",
    "\t\t\t\tself.action_prev = action_now\n",
    "\t\t\t\t\n",
    "\t\t\t\t# just a notice :\n",
    "\t\t\t\tfor k,v in reward_tmp.items():\n",
    "\t\t\t\t\t\tif v >= 1.0 or v <= -1.0:\n",
    "\t\t\t\t\t\t\t\tprint(\"reward for criteria '%s' is %f\" % (k, v))\n",
    "\t\t\t\t\n",
    "\t\t\t\t# sum up rewards\n",
    "\t\t\t\treward_shaped = sum(reward_tmp.values())\n",
    "\t\t\t\treturn np.clip(reward_shaped, -0.9, 0.9), reward_tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleAgent as reward (psudo-pretraining) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShaping_Simple:\n",
    "\tdef __init__(self):\n",
    "\t\tself.sa = SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.sa = SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "\n",
    "\tdef shape_it(self, obs_now, action_now):\n",
    "\t\taction_simple = self.sa.act(obs_now, [])\n",
    "\t\t#off policy by using simple agent action\n",
    "\t\tif action_now == action_simple:\n",
    "\t\t\treturn 0.001, action_simple\n",
    "\t\telse:\n",
    "\t\t\treturn -0.001, action_simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one-hot encoded feature layers from obs data.\n",
    "\n",
    "def new_featurize(obs):\n",
    "\n",
    "\tBOARD_SIZE = 11\n",
    "\n",
    "\tshape = (BOARD_SIZE, BOARD_SIZE, 1)\n",
    "\n",
    "\tdef get_matrix(board, key):\n",
    "\t\tres = board[key]\n",
    "\t\treturn res.reshape(shape).astype(np.float32)\n",
    "\n",
    "\tdef get_map(board, item):\n",
    "\t\tmap1 = np.zeros(shape)\n",
    "\t\tmap1[board == item] = 1\n",
    "\t\treturn map1\n",
    "\n",
    "\tdef get_multi(board, item):\n",
    "\t\tmap1 = np.zeros(shape)\n",
    "\t\tfor i in item:\n",
    "\t\t\tmap1[board == i] = 1\n",
    "\t\treturn map1\n",
    "\n",
    "#Makes bomb explosion pattern from location, strength and time. \n",
    "#Recursive due to possibility of a bomb blowing up other bombs before their time.\n",
    "\tdef bombSpread(bombMap,orgMap,bomb_life):\n",
    "\t\tbombMap1 = np.zeros_like(bombMap)\n",
    "\t\tfor x in range(11):\n",
    "\t\t\tfor y in range(11):\n",
    "\t\t\t\tif (bomb_life[x][y] > 0): #bomb found\n",
    "\t\t\t\t\tstrength = bombMap[x][y]\n",
    "\t\t\t\t\ttime = bomb_life[x][y]\n",
    "\t\t\t\t\tbombMap1[x][y] = time\n",
    "\t\t\t\t\tfor di in [(0,1),(1,0),(-1,0),(0,-1)]: # for each direction\n",
    "\t\t\t\t\t\ttravel = 1\n",
    "\t\t\t\t\t\twhile ((travel*di[0] + x <= 10) & (travel*di[0] + x >= 0) & #travel in-bounds, strength times\n",
    "\t\t\t\t\t\t\t\t(travel*di[1] + y <= 10) & (travel*di[1] + y >= 0) &\n",
    "\t\t\t\t\t\t\t\t(travel < strength)):\n",
    "\t\t\t\t\t\t\t#met other bomb, and timer is longer than current\n",
    "\t\t\t\t\t\t\tif (bomb_life[x+travel*di[0]][y+travel*di[1]] > time):\n",
    "\t\t\t\t\t\t\t\t# copy bomblife, update and rerun bombspread, return from here.\n",
    "\t\t\t\t\t\t\t\tbomb_life2 = np.copy(bomb_life)\n",
    "\t\t\t\t\t\t\t\tbomb_life2[x+travel*di[0]][y+travel*di[1]] = time\n",
    "\t\t\t\t\t\t\t\treturn bombSpread(bombMap,orgMap,bomb_life2)\n",
    "\t\t\t\t\t\t\tbombMap1[x+travel*di[0]][y+travel*di[1]] = time\n",
    "\t\t\t\t\t\t\tif (orgMap[x+travel*di[0]][y+travel*di[1]]) in [1,2]: #unpassable\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\ttravel += 1\n",
    "\t\treturn bombMap1 \n",
    "\n",
    "### Centering (11 x 11 view)    \n",
    "\tdef shift_board(arr, pos, fill_value=2):\n",
    "\t\tdx = 5 - pos[1]\n",
    "\t\tdy = 5 - pos[0]\n",
    "\t\tresult = np.empty_like(arr)\n",
    "\t\tif dy > 0:\n",
    "\t\t\tresult[:dy] = fill_value\n",
    "\t\t\tresult[dy:] = arr[:-dy]\n",
    "\t\telif dy < 0:\n",
    "\t\t\tresult[dy:] = fill_value\n",
    "\t\t\tresult[:dy] = arr[-dy:]\n",
    "\t\telse:\n",
    "\t\t\tresult = arr\n",
    "\t\tarr= np.transpose(result)\n",
    "\t\tresult = np.empty_like(arr)\n",
    "\t\tresult = np.empty_like(arr)\n",
    "\t\tif dx > 0:\n",
    "\t\t\tresult[:dx] = fill_value\n",
    "\t\t\tresult[dx:] = arr[:-dx]\n",
    "\t\telif dx < 0:\n",
    "\t\t\tresult[dx:] = fill_value\n",
    "\t\t\tresult[:dx] = arr[-dx:]\n",
    "\t\telse:\n",
    "\t\t\tresult = arr\n",
    "\t\tresult = np.transpose(result)\n",
    "\t\treturn result    \n",
    "\n",
    "\tposition = obs[\"position\"]\n",
    "\tmy_position = np.zeros(shape)\n",
    "\tmy_position[position[0], position[1], 0] = 1    \n",
    "    \n",
    "\tif CENTERED:\n",
    "\t\tobs['board'] \t\t\t\t= shift_board(obs['board'], position, fill_value=2)\n",
    "\t\tobs['bomb_blast_strength'] \t= shift_board(obs['bomb_blast_strength'], position, fill_value=0)\n",
    "\t\tobs['bomb_life'] \t\t\t= shift_board(obs['bomb_life'], position, fill_value=0)\n",
    "\t\tposition = (5,5)# will always be this when centered    \n",
    "    \n",
    "    \n",
    "\tboard = get_matrix(obs, 'board')\n",
    "        \n",
    "\tpath_map\t\t= get_map(board, 0)\t\t\t\t\t# Empty space\n",
    "\trigid_map\t\t= get_map(board, 1)\t\t\t\t\t# Rigid = 1\n",
    "\twood_map\t\t= get_map(board, 2)\t\t\t\t\t# Wood = 2\n",
    "\tflames_map\t\t= get_map(board, 4)\t\t\t\t\t# Flames = 4\n",
    "\tupgrade_map\t\t= get_multi(board, [6,7,8])\n",
    "\n",
    "\tenemies = np.zeros(shape)\n",
    "\tfor enemy in obs[\"enemies\"]:\n",
    "\t\tenemies[board == enemy.value] = 1\n",
    "\n",
    "\tbomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "\tbomb_life\t\t    = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "\tammo\t\t\t= np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"ammo\"])\n",
    "\tblast_strength  = np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"blast_strength\"])\n",
    "\tcan_kick\t\t= np.full((BOARD_SIZE, BOARD_SIZE, 1), int(obs[\"can_kick\"]))\n",
    "\n",
    "\tmaps = []\n",
    "    \n",
    "\tif not CENTERED:\n",
    "\t\tmaps = [my_position]\n",
    "\n",
    "\tif BLASTPAT:\n",
    "    #blast pattern bomb map with count down.\n",
    "\t\tbomb_map = bombSpread(obs['bomb_blast_strength'],obs['board'],obs['bomb_life']) \n",
    "\t\tbomb_map = bomb_map.reshape(shape).astype(np.float32)        \n",
    "\t\tmaps += [bomb_map]\n",
    "\telse:\n",
    "\t\tbomb_map = get_map(board, 3)\n",
    "\t\tmaps +=[bomb_map,bomb_blast_strength,bomb_life]     \n",
    "        \n",
    "\tmaps += [enemies,\n",
    "\t\t\tpath_map,\n",
    "\t\t\trigid_map,\n",
    "\t\t\twood_map,\n",
    "\t\t\tflames_map,\n",
    "\t\t\tupgrade_map,\n",
    "\t\t\tammo,\n",
    "\t\t\tblast_strength,\n",
    "\t\t\tcan_kick]\n",
    "\tobs = np.concatenate(maps, axis=2)  \n",
    "\n",
    "\treturn obs.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEnv(OpenAIGym):\n",
    "\tdef __init__(self, gym, visualize=False):\n",
    "\t\tself.rewardShaping = RewardShaping()\n",
    "\t\tself.rewardShaping_Simple =RewardShaping_Simple()\n",
    "\t\tself.gym = gym\n",
    "\t\tself.visualize = visualize\n",
    "\t\tself.ecount = 0\n",
    "\t\tself.rewards = []\n",
    "\t\tself.take_actions = [0,0,0,0,0,0]\n",
    "\n",
    "\tdef execute(self, action):\n",
    "\t\tif self.visualize:\n",
    "\t\t\tself.gym.render()\n",
    "\n",
    "\t\tobs = self.gym.get_observations() #old state\n",
    "\n",
    "        #Simple agent reward shaping (Off policy)\n",
    "\t\tif (SAREWARD):\n",
    "\t\t\tagent_reward1, test_action = self.rewardShaping_Simple.shape_it(obs[self.gym.training_agent], action)\n",
    "\t\t\taction = test_action # Off policy, simple agent in control.\n",
    "            \n",
    "        #Pommerman x reward shaping\n",
    "\t\tif (PXREWARD):\n",
    "\t\t\tagent_reward1, _ = self.rewardShaping.shape_it(obs[self.gym.training_agent], action)            \n",
    "        \n",
    "\t\tactions = self.unflatten_action(action=action)\n",
    "\t\tself.take_actions[actions] += 1\n",
    "\n",
    "\t\tall_actions = self.gym.act(obs)\n",
    "\t\tall_actions.insert(self.gym.training_agent, actions)\n",
    "\t\tstate, reward, terminal, _ = self.gym.step(all_actions)\n",
    "\t\tagent_state = new_featurize(state[self.gym.training_agent]) #state changed by featurize\n",
    "\t\tagent_reward = reward[self.gym.training_agent]\n",
    "\t\t\t\t\n",
    "\t\t# End of episode:\n",
    "\t\tif (terminal):\n",
    "\t\t\t\tself.ecount += 1\n",
    "\t\t\t\tself.rewards.append(agent_reward)\n",
    "                #random shuffle\n",
    "\t\t\t\tif (RANDOMSTART): \n",
    "\t\t\t\t\tglobal agents\n",
    "\t\t\t\t\tglobal agent_id\n",
    "\t\t\t\t\tglobal env\n",
    "\t\t\t\t\trandom.shuffle(agents)\n",
    "\t\t\t\t\tfor idx,item in enumerate(agents):\n",
    "\t\t\t\t\t\titem.set_agent_id(idx)\n",
    "\t\t\t\t\t\tif(type(item) == TensorforceAgent):\n",
    "\t\t\t\t\t\t\tagent_id= idx\n",
    "\t\t\t\t\tenv.set_training_agent(agents[agent_id].agent_id)\n",
    "\t\t\t\t\tenv.set_agents(agents)\n",
    "\t\t\t\tne = 10 # evaluate every ne episodes\n",
    "                #Log env reward: (-1, 0, +1)\n",
    "\t\t\t\tif not self.ecount % ne:\n",
    "\t\t\t\t\t\tprint(\"Finished episodes: {ep}; mean/median reward over last {ne} episodes: {meanr} / {medianr}; min reward:{minr}; max reward:{maxr}\"\n",
    "\t\t\t\t\t\t\t.format(ep=self.ecount,\n",
    "\t\t\t\t\t\t\t\tne = ne,\n",
    "\t\t\t\t\t\t\t\tmeanr=np.mean(self.rewards[-ne:]),\n",
    "\t\t\t\t\t\t\t\tmedianr=np.median(self.rewards[-ne:]),\n",
    "\t\t\t\t\t\t\t\tminr=np.min(self.rewards[-ne:]),\n",
    "\t\t\t\t\t\t\t\tmaxr=np.max(self.rewards[-ne:])))\n",
    "\t\t\t\t\t\twith open('results/log_conv_ppo_lre3-ras.csv', 'a') as the_file2:\n",
    "\t\t\t\t\t\t\twriter = csv.writer(the_file2,delimiter = ',')\n",
    "\t\t\t\t\t\t\tline1 = [self.ecount,\n",
    "\t\t\t\t\t\t\t\t\tround(np.mean(self.rewards[-ne:]), 2),\n",
    "\t\t\t\t\t\t\t\t\tnp.median(self.rewards[-ne:]),\n",
    "\t\t\t\t\t\t\t\t\tnp.min(self.rewards[-ne:]),\n",
    "\t\t\t\t\t\t\t\t\tnp.max(self.rewards[-ne:])]\n",
    "\t\t\t\t\t\t\twriter.writerow(line1)\n",
    "\t\t\t\t# REWARD shaping:\n",
    "\t\tif (PXREWARD or SAREWARD):\n",
    "\t\t\tagent_reward = agent_reward1                     \n",
    "                    \n",
    "\t\treturn agent_state, terminal, agent_reward\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tobs = self.gym.reset()\n",
    "\t\tself.rewardShaping.reset()\n",
    "\t\tself.rewardShaping_Simple.reset()\n",
    "\t\tagent_obs = new_featurize(obs[agent_id])\n",
    "\t\treturn agent_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging & Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/log_conv_ppo_lre3-ras.csv', 'w') as the_file:\n",
    "\theader = [\"episodes\",\"mean timesteps\", \"mean reward\", \"median reward\", \"min reward\", \"max reward\", \"mc action\"]\n",
    "\twriter = csv.writer(the_file,delimiter = ',')\n",
    "\twriter.writerow(header)\n",
    "\t\t\n",
    "with open('results/log_conv_ppo_lre3-ras2.csv', 'w') as the_file2:\n",
    "\theader = [\"episodes\", \"mean reward\", \"median reward\", \"min reward\", \"max reward\"]\n",
    "\twriter = csv.writer(the_file2,delimiter = ',')\n",
    "\twriter.writerow(header)\t\t \n",
    "\n",
    "\n",
    "# Callback function to print episode statistics\n",
    "def episode_finished(r):\n",
    "\tne = 10 # evaluate every ne episodes\n",
    "\n",
    "\tif not r.episode % ne:\n",
    "\t\tprint(\"Finished episodes: {ep} mean timesteps: {ts}; mean/median reward over last episodes: {meanr} / {medianr}; min reward:{minr}; max reward:{maxr}; taken actions:{tac}\"\n",
    "\t\t\t.format(ep=r.episode,\n",
    "\t\t\t\t\tts=round(np.mean(r.episode_timesteps), 2),\n",
    "\t\t\t\t\tne = ne,\n",
    "\t\t\t\t\tmeanr=round(np.mean(r.episode_rewards[-ne:]), 2),\n",
    "\t\t\t\t\tmedianr=round(np.median(r.episode_rewards[-ne:]), 2),\n",
    "\t\t\t\t\tminr=round(np.min(r.episode_rewards[-ne:]), 2),\n",
    "\t\t\t\t\tmaxr=round(np.max(r.episode_rewards[-ne:]), 2),\n",
    "\t\t\t\t\ttac=wrapped_env.take_actions))\n",
    "\n",
    "\t\twith open('results/log_conv_ppo_lre3-ras.csv', 'a') as the_file:\n",
    "\t\t\twriter = csv.writer(the_file,delimiter = ',')\n",
    "\t\t\tline2 = [r.episode,\n",
    "\t\t\t\t\tnp.mean(r.episode_timesteps[-ne:]),\n",
    "\t\t\t\t\tnp.mean(r.episode_rewards[-ne:]),\n",
    "\t\t\t\t\tnp.median(r.episode_rewards[-ne:]),\n",
    "\t\t\t\t\tnp.min(r.episode_rewards[-ne:]),\n",
    "\t\t\t\t\tnp.max(r.episode_rewards[-ne:]),\n",
    "\t\t\t\t\twrapped_env.take_actions]\n",
    "\t\t\twriter.writerow(line2)\n",
    "\n",
    "\tif not r.episode % 1000:\n",
    "\t\tprint(\"saving model\")\n",
    "\t\tagent.save_model(directory=\"results/ppo_lre3-ras_model/\")\n",
    "\treturn True    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and the environment\n",
    "\n",
    "wrapped_env = WrappedEnv(env, False) # set to false to disable rendering\n",
    "\n",
    "#agent.restore_model(directory=\"/content/drive/My Drive/DeepLearning/Results/\")\n",
    "\n",
    "# Create the Tensorforce runner\n",
    "# change agent here\n",
    "runner = Runner(agent=agent, environment=wrapped_env)\n",
    "\n",
    "# Start learning for n episodes\n",
    "runner.run(episodes=EPISODES, max_episode_timesteps=2000, episode_finished=episode_finished)\n",
    "\n",
    "try:\n",
    "\trunner.close()\n",
    "except AttributeError as e:\n",
    "\tpass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
